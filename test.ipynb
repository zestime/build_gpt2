{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890f9543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading GPT-2 (small) model...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load model\n",
    "print(\"üöÄ Loading GPT-2 (small) model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "def show_statistics(model):\n",
    "    records = []\n",
    "\n",
    "    print(\"üîç Computing statistics for each parameter...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.numel() == 0:\n",
    "            continue\n",
    "        records.append({\n",
    "            \"parameter_name\": name,\n",
    "            \"min\": param.min().item(),\n",
    "            \"max\": param.max().item(),\n",
    "            \"avg\": param.mean().item(),\n",
    "            \"std\": param.std().item(),\n",
    "            \"numel\": param.numel()\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Reorder columns for readability\n",
    "    df = df[[\"parameter_name\", \"min\", \"max\", \"avg\", \"std\", \"numel\"]]\n",
    "\n",
    "    # Pretty-print table (same format as requested)\n",
    "    print(\"\\n\" + \"=\"*105)\n",
    "    print(f\"{'Parameter Name':<45} | {'Min':>10} | {'Max':>10} | {'Avg':>10} | {'Std':>10} | {'#Params':>8}\")\n",
    "    print(\"-\"*105)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        print(\n",
    "            f\"{row['parameter_name']:<45} | \"\n",
    "            f\"{row['min']:10.4e} | \"\n",
    "            f\"{row['max']:10.4e} | \"\n",
    "            f\"{row['avg']:10.4e} | \"\n",
    "            f\"{row['std']:10.4e} | \"\n",
    "            f\"{row['numel']:8,}\"\n",
    "        )\n",
    "\n",
    "    print(\"-\"*105)\n",
    "    print(f\"‚úÖ Total parameters: {df['numel'].sum():,}\")\n",
    "    print(f\"‚úÖ DataFrame shape: {df.shape} (rows=parameters, cols=stats)\")\n",
    "\n",
    "    # Optional: Save to file\n",
    "    # df.to_csv(\"gpt2_weight_stats.csv\", index=False)\n",
    "    # df.to_excel(\"gpt2_weight_stats.xlsx\", index=False)\n",
    "\n",
    "    # ‚úÖ Now `df` is ready for analysis!\n",
    "    # Example queries:\n",
    "    print(\"\\nüí° Example usage:\")\n",
    "    print(f\"- Mean std across all params: {df['std'].mean():.4f}\")\n",
    "    print(f\"- Layer with largest weight magnitude (max |max|):\")\n",
    "    abs_max_row = df.loc[df[['min', 'max']].abs().max(axis=1).idxmax()]\n",
    "    print(f\"  ‚Üí {abs_max_row['parameter_name']} (max={abs_max_row['max']:.4f}, min={abs_max_row['min']:.4f})\")\n",
    "\n",
    "    # Bonus: Add layer-level grouping (e.g., 'h.0', 'h.1', etc.)\n",
    "    def extract_layer_group(name):\n",
    "        if name.startswith(\"transformer.h.\"):\n",
    "            # e.g., 'transformer.h.5.mlp.c_proj.weight' ‚Üí 'h.5'\n",
    "            return \".\".join(name.split(\".\")[1:3])  # ['transformer', 'h', '5', ...] ‚Üí 'h.5'\n",
    "        elif name.startswith(\"transformer.wte\") or name.startswith(\"transformer.wpe\"):\n",
    "            return \"embeddings\"\n",
    "        elif name.startswith(\"transformer.ln_f\"):\n",
    "            return \"final_ln\"\n",
    "        elif name == \"lm_head.weight\":\n",
    "            return \"lm_head\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "    df[\"layer_group\"] = df[\"parameter_name\"].apply(extract_layer_group)\n",
    "\n",
    "    # Optional: Per-layer-group summary (mean of stats weighted by numel)\n",
    "    print(\"\\nüìä Per-layer-group aggregated stats (numel-weighted avg):\")\n",
    "    layer_summary = df.groupby(\"layer_group\").apply(\n",
    "        lambda g: pd.Series({\n",
    "            \"total_params\": g[\"numel\"].sum(),\n",
    "            \"avg_min\": np.average(g[\"min\"], weights=g[\"numel\"]),\n",
    "            \"avg_max\": np.average(g[\"max\"], weights=g[\"numel\"]),\n",
    "            \"avg_avg\": np.average(g[\"avg\"], weights=g[\"numel\"]),\n",
    "            \"avg_std\": np.average(g[\"std\"], weights=g[\"numel\"]),\n",
    "        })\n",
    "    ).round(6)\n",
    "\n",
    "    print(layer_summary.to_string(float_format=\"{:.6e}\".format))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fafb290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "qwen_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b35d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Computing statistics for each parameter...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'parameter_name': 'model.embed_tokens.weight',\n",
       "  'min': -0.220703125,\n",
       "  'max': 0.24609375,\n",
       "  'avg': -2.6464462280273438e-05,\n",
       "  'std': 0.021728515625,\n",
       "  'numel': 388956160}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    records = []\n",
    "\n",
    "    print(\"üîç Computing statistics for each parameter...\")\n",
    "    for name, param in qwen_model.named_parameters():\n",
    "        if param.numel() == 0:\n",
    "            continue\n",
    "        \n",
    "        records.append({\n",
    "            \"parameter_name\": name,\n",
    "            \"min\": param.min().item(),\n",
    "            \"max\": param.max().item(),\n",
    "            \"avg\": param.mean().item(),\n",
    "            \"std\": param.std().item(),\n",
    "            \"numel\": param.numel()\n",
    "        })\n",
    "        break\n",
    "    records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61efb28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Computing statistics for each parameter...\n",
      "\n",
      "=========================================================================================================\n",
      "Parameter Name                                |        Min |        Max |        Avg |        Std |  #Params\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "model.embed_tokens.weight                     | -2.2070e-01 | 2.4609e-01 | -2.6464e-05 | 2.1729e-02 | 388,956,160\n",
      "model.layers.0.self_attn.q_proj.weight        | -5.8984e-01 | 4.3945e-01 | 6.7651e-06 | 2.2949e-02 | 10,485,760\n",
      "model.layers.0.self_attn.k_proj.weight        | -2.9297e-01 | 2.4805e-01 | 1.1921e-05 | 2.4170e-02 | 2,621,440\n",
      "model.layers.0.self_attn.v_proj.weight        | -1.6895e-01 | 1.4648e-01 | -9.9540e-06 | 2.2705e-02 | 2,621,440\n",
      "model.layers.0.self_attn.o_proj.weight        | -5.1172e-01 | 5.1172e-01 | -6.7055e-06 | 2.1362e-02 | 10,485,760\n",
      "model.layers.0.self_attn.q_norm.weight        | -7.2021e-03 | 3.7500e+00 | 1.7891e+00 | 6.0156e-01 |      128\n",
      "model.layers.0.self_attn.k_norm.weight        | -1.6357e-02 | 4.4000e+01 | 2.3438e+00 | 3.9219e+00 |      128\n",
      "model.layers.0.mlp.gate_proj.weight           | -4.2969e-01 | 5.1953e-01 | 7.4208e-06 | 2.3315e-02 | 24,903,680\n",
      "model.layers.0.mlp.up_proj.weight             | -3.2812e-01 | 3.0664e-01 | -2.9653e-06 | 2.1729e-02 | 24,903,680\n",
      "model.layers.0.mlp.down_proj.weight           | -5.4297e-01 | 5.6641e-01 | -8.1062e-06 | 2.2949e-02 | 24,903,680\n",
      "model.layers.0.input_layernorm.weight         | -1.0449e-01 | 2.3926e-01 | 2.2827e-02 | 1.1230e-02 |    2,560\n",
      "model.layers.0.post_attention_layernorm.weight | -1.3184e-01 | 3.6328e-01 | 2.0801e-01 | 3.8086e-02 |    2,560\n",
      "model.layers.1.self_attn.q_proj.weight        | -3.7305e-01 | 3.8281e-01 | 1.4976e-06 | 2.1362e-02 | 10,485,760\n",
      "model.layers.1.self_attn.k_proj.weight        | -2.2949e-01 | 2.0898e-01 | 2.6971e-06 | 2.2217e-02 | 2,621,440\n",
      "model.layers.1.self_attn.v_proj.weight        | -1.4648e-01 | 1.5332e-01 | 3.6210e-06 | 2.2949e-02 | 2,621,440\n",
      "model.layers.1.self_attn.o_proj.weight        | -4.0820e-01 | 4.0234e-01 | -3.0994e-06 | 2.1606e-02 | 10,485,760\n",
      "model.layers.1.self_attn.q_norm.weight        | 4.8584e-02 | 4.3125e+00 | 1.5703e+00 | 5.0000e-01 |      128\n",
      "model.layers.1.self_attn.k_norm.weight        | -1.0078e+00 | 1.2062e+01 | 1.7500e+00 | 1.1484e+00 |      128\n",
      "model.layers.1.mlp.gate_proj.weight           | -3.9453e-01 | 2.5781e-01 | -4.8161e-05 | 2.2583e-02 | 24,903,680\n",
      "model.layers.1.mlp.up_proj.weight             | -2.9297e-01 | 3.0078e-01 | -2.0415e-06 | 1.5259e-02 | 24,903,680\n",
      "model.layers.1.mlp.down_proj.weight           | -6.8750e-01 | 7.1484e-01 | -3.2783e-06 | 1.6235e-02 | 24,903,680\n",
      "model.layers.1.input_layernorm.weight         | -5.7861e-02 | 2.0898e-01 | 3.8086e-02 | 1.7456e-02 |    2,560\n",
      "model.layers.1.post_attention_layernorm.weight | -5.7422e-01 | 4.0938e+00 | 6.2500e-01 | 1.9629e-01 |    2,560\n",
      "model.layers.2.self_attn.q_proj.weight        | -3.3789e-01 | 2.7344e-01 | -5.2527e-07 | 2.2095e-02 | 10,485,760\n",
      "model.layers.2.self_attn.k_proj.weight        | -4.1016e-01 | 3.6328e-01 | -2.6077e-06 | 2.2827e-02 | 2,621,440\n",
      "model.layers.2.self_attn.v_proj.weight        | -2.1973e-01 | 1.9727e-01 | 1.8120e-05 | 2.3804e-02 | 2,621,440\n",
      "model.layers.2.self_attn.o_proj.weight        | -5.6641e-01 | 3.6914e-01 | -3.3528e-06 | 2.2095e-02 | 10,485,760\n",
      "model.layers.2.self_attn.q_norm.weight        | 1.6113e-02 | 3.4531e+00 | 1.5703e+00 | 4.4336e-01 |      128\n",
      "model.layers.2.self_attn.k_norm.weight        | -3.0029e-02 | 6.8750e+00 | 1.5703e+00 | 7.5391e-01 |      128\n",
      "model.layers.2.mlp.gate_proj.weight           | -2.3828e-01 | 3.0469e-01 | -1.5259e-05 | 1.8555e-02 | 24,903,680\n",
      "model.layers.2.mlp.up_proj.weight             | -4.2969e-01 | 5.1953e-01 | -4.2021e-06 | 1.6968e-02 | 24,903,680\n",
      "model.layers.2.mlp.down_proj.weight           | -7.1875e-01 | 7.7344e-01 | -7.8231e-07 | 1.6357e-02 | 24,903,680\n",
      "model.layers.2.input_layernorm.weight         | -8.6914e-02 | 2.8125e-01 | 6.6406e-02 | 2.7100e-02 |    2,560\n",
      "model.layers.2.post_attention_layernorm.weight | -1.1253e-04 | 5.5625e+00 | 8.3594e-01 | 2.8906e-01 |    2,560\n",
      "model.layers.3.self_attn.q_proj.weight        | -4.4531e-01 | 3.1250e-01 | -5.3048e-06 | 2.2095e-02 | 10,485,760\n",
      "model.layers.3.self_attn.k_proj.weight        | -3.5156e-01 | 3.3789e-01 | 6.4373e-06 | 2.2095e-02 | 2,621,440\n",
      "model.layers.3.self_attn.v_proj.weight        | -1.9336e-01 | 1.6602e-01 | 7.7486e-06 | 2.3926e-02 | 2,621,440\n",
      "model.layers.3.self_attn.o_proj.weight        | -3.4180e-01 | 4.1211e-01 | 9.0804e-08 | 2.2339e-02 | 10,485,760\n",
      "model.layers.3.self_attn.q_norm.weight        | -2.4219e-01 | 2.8438e+00 | 1.6719e+00 | 4.9023e-01 |      128\n",
      "model.layers.3.self_attn.k_norm.weight        | -2.2705e-02 | 3.7969e+00 | 1.7656e+00 | 6.1719e-01 |      128\n",
      "model.layers.3.mlp.gate_proj.weight           | -3.5156e-01 | 4.8438e-01 | -5.9128e-05 | 2.4780e-02 | 24,903,680\n",
      "model.layers.3.mlp.up_proj.weight             | -4.5312e-01 | 3.9062e-01 | 3.0398e-06 | 1.8188e-02 | 24,903,680\n",
      "model.layers.3.mlp.down_proj.weight           | -7.2656e-01 | 5.6641e-01 | -1.4249e-07 | 1.8799e-02 | 24,903,680\n",
      "model.layers.3.input_layernorm.weight         | -1.7090e-01 | 3.0273e-01 | 8.3008e-02 | 2.8687e-02 |    2,560\n",
      "model.layers.3.post_attention_layernorm.weight | -2.1875e-01 | 4.4062e+00 | 7.4219e-01 | 1.9141e-01 |    2,560\n",
      "model.layers.4.self_attn.q_proj.weight        | -3.0273e-01 | 4.2969e-01 | -1.8533e-07 | 2.2461e-02 | 10,485,760\n",
      "model.layers.4.self_attn.k_proj.weight        | -7.3828e-01 | 5.4297e-01 | 2.2054e-05 | 2.2705e-02 | 2,621,440\n",
      "model.layers.4.self_attn.v_proj.weight        | -1.8555e-01 | 1.7285e-01 | 3.6240e-05 | 2.4170e-02 | 2,621,440\n",
      "model.layers.4.self_attn.o_proj.weight        | -4.8633e-01 | 3.8672e-01 | -5.4836e-06 | 2.2827e-02 | 10,485,760\n",
      "model.layers.4.self_attn.q_norm.weight        | -1.1902e-02 | 3.2500e+00 | 1.6641e+00 | 5.3125e-01 |      128\n",
      "model.layers.4.self_attn.k_norm.weight        | -4.3640e-03 | 1.2500e+01 | 1.7734e+00 | 1.2109e+00 |      128\n",
      "model.layers.4.mlp.gate_proj.weight           | -3.0469e-01 | 3.1836e-01 | -5.6982e-05 | 3.0884e-02 | 24,903,680\n",
      "model.layers.4.mlp.up_proj.weight             | -3.7305e-01 | 2.9102e-01 | -7.7765e-08 | 2.0142e-02 | 24,903,680\n",
      "model.layers.4.mlp.down_proj.weight           | -6.5625e-01 | 6.3672e-01 | 8.3447e-06 | 2.0386e-02 | 24,903,680\n",
      "model.layers.4.input_layernorm.weight         | -1.0156e-01 | 3.7500e-01 | 1.1914e-01 | 2.8931e-02 |    2,560\n",
      "model.layers.4.post_attention_layernorm.weight | -7.5000e-01 | 2.7969e+00 | 6.3672e-01 | 1.3574e-01 |    2,560\n",
      "model.layers.5.self_attn.q_proj.weight        | -4.4922e-01 | 3.7500e-01 | -2.3544e-06 | 2.2461e-02 | 10,485,760\n",
      "model.layers.5.self_attn.k_proj.weight        | -3.6719e-01 | 3.3594e-01 | 6.4969e-06 | 2.2339e-02 | 2,621,440\n",
      "model.layers.5.self_attn.v_proj.weight        | -1.6699e-01 | 1.5723e-01 | 1.8358e-05 | 2.4048e-02 | 2,621,440\n",
      "model.layers.5.self_attn.o_proj.weight        | -4.2383e-01 | 4.1797e-01 | -1.9968e-06 | 2.1973e-02 | 10,485,760\n",
      "model.layers.5.self_attn.q_norm.weight        | 1.9897e-02 | 4.7188e+00 | 1.0547e+00 | 5.9375e-01 |      128\n",
      "model.layers.5.self_attn.k_norm.weight        | -2.7084e-04 | 1.7625e+01 | 3.3281e+00 | 2.0625e+00 |      128\n",
      "model.layers.5.mlp.gate_proj.weight           | -9.6875e-01 | 4.5312e-01 | -4.3809e-06 | 2.8931e-02 | 24,903,680\n",
      "model.layers.5.mlp.up_proj.weight             | -4.8047e-01 | 6.7188e-01 | 4.6790e-06 | 2.1362e-02 | 24,903,680\n",
      "model.layers.5.mlp.down_proj.weight           | -6.8359e-01 | 6.5625e-01 | -4.7311e-07 | 2.0996e-02 | 24,903,680\n",
      "model.layers.5.input_layernorm.weight         | -1.2695e-01 | 4.3164e-01 | 1.2305e-01 | 2.9053e-02 |    2,560\n",
      "model.layers.5.post_attention_layernorm.weight | -4.3750e-01 | 1.6094e+00 | 4.6680e-01 | 8.0078e-02 |    2,560\n",
      "model.layers.6.self_attn.q_proj.weight        | -9.8047e-01 | 7.6562e-01 | 2.5928e-06 | 2.3071e-02 | 10,485,760\n",
      "model.layers.6.self_attn.k_proj.weight        | -3.9648e-01 | 3.9648e-01 | -9.5963e-06 | 2.2461e-02 | 2,621,440\n",
      "model.layers.6.self_attn.v_proj.weight        | -2.2461e-01 | 2.5781e-01 | 1.4722e-05 | 2.4048e-02 | 2,621,440\n",
      "model.layers.6.self_attn.o_proj.weight        | -5.3516e-01 | 5.2344e-01 | -6.5863e-06 | 2.2705e-02 | 10,485,760\n",
      "model.layers.6.self_attn.q_norm.weight        | 2.1606e-02 | 2.9531e+00 | 1.5938e+00 | 4.7461e-01 |      128\n",
      "model.layers.6.self_attn.k_norm.weight        | 5.9509e-04 | 8.2500e+00 | 1.6719e+00 | 8.5156e-01 |      128\n",
      "model.layers.6.mlp.gate_proj.weight           | -4.7656e-01 | 3.8477e-01 | -3.1471e-05 | 2.7222e-02 | 24,903,680\n",
      "model.layers.6.mlp.up_proj.weight             | -4.3555e-01 | 4.9219e-01 | 1.7658e-06 | 2.1851e-02 | 24,903,680\n",
      "model.layers.6.mlp.down_proj.weight           | -9.8047e-01 | 9.9609e-01 | 4.8876e-06 | 2.1851e-02 | 24,903,680\n",
      "model.layers.6.input_layernorm.weight         | -2.1777e-01 | 4.7266e-01 | 1.7188e-01 | 3.8818e-02 |    2,560\n",
      "model.layers.6.post_attention_layernorm.weight | -4.3164e-01 | 7.7188e+00 | 4.9805e-01 | 1.6992e-01 |    2,560\n",
      "model.layers.7.self_attn.q_proj.weight        | -3.5156e-01 | 3.8086e-01 | 8.4043e-06 | 2.2339e-02 | 10,485,760\n",
      "model.layers.7.self_attn.k_proj.weight        | -5.2344e-01 | 4.6875e-01 | -2.3991e-06 | 2.2461e-02 | 2,621,440\n",
      "model.layers.7.self_attn.v_proj.weight        | -1.4453e-01 | 1.5430e-01 | 1.3769e-05 | 2.5757e-02 | 2,621,440\n",
      "model.layers.7.self_attn.o_proj.weight        | -5.9766e-01 | 3.7305e-01 | -2.9951e-06 | 2.2461e-02 | 10,485,760\n",
      "model.layers.7.self_attn.q_norm.weight        | 2.1680e-01 | 3.1406e+00 | 1.7578e+00 | 4.4922e-01 |      128\n",
      "model.layers.7.self_attn.k_norm.weight        | 8.0078e-02 | 3.4062e+00 | 1.7969e+00 | 5.2344e-01 |      128\n",
      "model.layers.7.mlp.gate_proj.weight           | -4.0430e-01 | 5.5859e-01 | -4.9114e-05 | 2.6978e-02 | 24,903,680\n",
      "model.layers.7.mlp.up_proj.weight             | -5.4297e-01 | 5.5078e-01 | 2.0117e-06 | 2.2095e-02 | 24,903,680\n",
      "model.layers.7.mlp.down_proj.weight           | -5.7422e-01 | 9.4922e-01 | -3.3341e-07 | 2.1973e-02 | 24,903,680\n",
      "model.layers.7.input_layernorm.weight         | -3.2997e-04 | 1.0234e+00 | 2.2754e-01 | 7.3730e-02 |    2,560\n",
      "model.layers.7.post_attention_layernorm.weight | -3.2617e-01 | 2.8125e+00 | 5.5078e-01 | 1.1230e-01 |    2,560\n",
      "model.layers.8.self_attn.q_proj.weight        | -3.3594e-01 | 3.5156e-01 | 6.1393e-06 | 2.2583e-02 | 10,485,760\n",
      "model.layers.8.self_attn.k_proj.weight        | -3.8672e-01 | 3.4961e-01 | 1.4126e-05 | 2.2827e-02 | 2,621,440\n",
      "model.layers.8.self_attn.v_proj.weight        | -1.4258e-01 | 1.3477e-01 | 7.0930e-06 | 2.5635e-02 | 2,621,440\n",
      "model.layers.8.self_attn.o_proj.weight        | -2.9688e-01 | 4.5117e-01 | 8.4564e-07 | 2.2949e-02 | 10,485,760\n",
      "model.layers.8.self_attn.q_norm.weight        | 5.2734e-02 | 3.8438e+00 | 1.5703e+00 | 4.4922e-01 |      128\n",
      "model.layers.8.self_attn.k_norm.weight        | -5.9509e-03 | 9.3750e+00 | 1.6250e+00 | 9.1406e-01 |      128\n",
      "model.layers.8.mlp.gate_proj.weight           | -3.6133e-01 | 4.9414e-01 | 1.2159e-05 | 2.5146e-02 | 24,903,680\n",
      "model.layers.8.mlp.up_proj.weight             | -5.2734e-01 | 5.5078e-01 | -9.8348e-07 | 2.2583e-02 | 24,903,680\n",
      "model.layers.8.mlp.down_proj.weight           | -8.5938e-01 | 9.1797e-01 | 3.6061e-06 | 2.2583e-02 | 24,903,680\n",
      "model.layers.8.input_layernorm.weight         | -2.3804e-03 | 1.0078e+00 | 2.9883e-01 | 6.8848e-02 |    2,560\n",
      "model.layers.8.post_attention_layernorm.weight | -2.4121e-01 | 9.1797e-01 | 5.3906e-01 | 1.1084e-01 |    2,560\n",
      "model.layers.9.self_attn.q_proj.weight        | -3.3594e-01 | 5.5469e-01 | -6.3181e-06 | 2.1973e-02 | 10,485,760\n",
      "model.layers.9.self_attn.k_proj.weight        | -6.4062e-01 | 7.7344e-01 | 8.4937e-07 | 2.1973e-02 | 2,621,440\n",
      "model.layers.9.self_attn.v_proj.weight        | -1.5527e-01 | 1.5625e-01 | -1.5199e-05 | 2.5879e-02 | 2,621,440\n",
      "model.layers.9.self_attn.o_proj.weight        | -3.5547e-01 | 7.8125e-01 | -1.0610e-05 | 2.2827e-02 | 10,485,760\n",
      "model.layers.9.self_attn.q_norm.weight        | -1.4893e-02 | 3.3125e+00 | 1.6172e+00 | 4.9219e-01 |      128\n",
      "model.layers.9.self_attn.k_norm.weight        | -8.2397e-03 | 3.1406e+00 | 1.5703e+00 | 6.3672e-01 |      128\n",
      "model.layers.9.mlp.gate_proj.weight           | -4.6680e-01 | 4.7461e-01 | 5.0068e-05 | 2.7100e-02 | 24,903,680\n",
      "model.layers.9.mlp.up_proj.weight             | -4.4336e-01 | 4.1992e-01 | 4.2021e-06 | 2.1606e-02 | 24,903,680\n",
      "model.layers.9.mlp.down_proj.weight           | -4.4922e-01 | 6.6016e-01 | 6.8545e-07 | 2.1484e-02 | 24,903,680\n",
      "model.layers.9.input_layernorm.weight         | -1.4591e-04 | 1.1641e+00 | 3.0273e-01 | 6.9336e-02 |    2,560\n",
      "model.layers.9.post_attention_layernorm.weight | 2.3127e-05 | 1.2188e+00 | 6.1719e-01 | 1.5137e-01 |    2,560\n",
      "model.layers.10.self_attn.q_proj.weight       | -3.0469e-01 | 3.2227e-01 | -1.5125e-06 | 2.2705e-02 | 10,485,760\n",
      "model.layers.10.self_attn.k_proj.weight       | -3.0859e-01 | 4.9609e-01 | 1.1683e-05 | 2.2705e-02 | 2,621,440\n",
      "model.layers.10.self_attn.v_proj.weight       | -1.9043e-01 | 1.8164e-01 | -2.6375e-06 | 2.5635e-02 | 2,621,440\n",
      "model.layers.10.self_attn.o_proj.weight       | -5.4688e-01 | 4.3945e-01 | -3.4720e-06 | 2.3193e-02 | 10,485,760\n",
      "model.layers.10.self_attn.q_norm.weight       | 5.7678e-03 | 3.9688e+00 | 1.4844e+00 | 5.7812e-01 |      128\n",
      "model.layers.10.self_attn.k_norm.weight       | -8.9264e-04 | 9.3750e+00 | 1.5859e+00 | 9.6484e-01 |      128\n",
      "model.layers.10.mlp.gate_proj.weight          | -7.3828e-01 | 5.6641e-01 | 2.7537e-05 | 2.6489e-02 | 24,903,680\n",
      "model.layers.10.mlp.up_proj.weight            | -5.1953e-01 | 8.8281e-01 | 5.3942e-06 | 2.2095e-02 | 24,903,680\n",
      "model.layers.10.mlp.down_proj.weight          | -1.0156e+00 | 6.6797e-01 | 9.8348e-06 | 2.1973e-02 | 24,903,680\n",
      "model.layers.10.input_layernorm.weight        | -5.3406e-03 | 1.5547e+00 | 4.1406e-01 | 1.0010e-01 |    2,560\n",
      "model.layers.10.post_attention_layernorm.weight | 1.3232e-05 | 9.1406e-01 | 6.0156e-01 | 1.6406e-01 |    2,560\n",
      "model.layers.11.self_attn.q_proj.weight       | -3.7109e-01 | 3.1641e-01 | -1.2279e-05 | 2.2583e-02 | 10,485,760\n",
      "model.layers.11.self_attn.k_proj.weight       | -3.6523e-01 | 3.2031e-01 | 2.4736e-06 | 2.2949e-02 | 2,621,440\n",
      "model.layers.11.self_attn.v_proj.weight       | -1.5527e-01 | 1.8750e-01 | -4.1127e-06 | 2.6001e-02 | 2,621,440\n",
      "model.layers.11.self_attn.o_proj.weight       | -4.9023e-01 | 7.3438e-01 | -7.3388e-07 | 2.2949e-02 | 10,485,760\n",
      "model.layers.11.self_attn.q_norm.weight       | 1.5991e-02 | 4.7188e+00 | 1.7031e+00 | 5.5078e-01 |      128\n",
      "model.layers.11.self_attn.k_norm.weight       | -8.7738e-04 | 7.0938e+00 | 1.7578e+00 | 9.1406e-01 |      128\n",
      "model.layers.11.mlp.gate_proj.weight          | -5.5859e-01 | 6.4844e-01 | 4.8161e-05 | 2.5757e-02 | 24,903,680\n",
      "model.layers.11.mlp.up_proj.weight            | -6.1719e-01 | 5.2734e-01 | -5.2154e-06 | 2.2705e-02 | 24,903,680\n",
      "model.layers.11.mlp.down_proj.weight          | -8.7500e-01 | 4.7461e-01 | -1.6391e-06 | 2.2461e-02 | 24,903,680\n",
      "model.layers.11.input_layernorm.weight        | -2.3804e-03 | 9.4141e-01 | 2.9883e-01 | 6.2988e-02 |    2,560\n",
      "model.layers.11.post_attention_layernorm.weight | -4.7119e-02 | 1.0156e+00 | 6.0156e-01 | 1.7090e-01 |    2,560\n",
      "model.layers.12.self_attn.q_proj.weight       | -3.0469e-01 | 3.1836e-01 | 1.8533e-07 | 2.2827e-02 | 10,485,760\n",
      "model.layers.12.self_attn.k_proj.weight       | -4.8438e-01 | 4.5508e-01 | 3.4720e-06 | 2.2583e-02 | 2,621,440\n",
      "model.layers.12.self_attn.v_proj.weight       | -2.1289e-01 | 2.0605e-01 | -3.6478e-05 | 2.5757e-02 | 2,621,440\n",
      "model.layers.12.self_attn.o_proj.weight       | -7.5391e-01 | 6.4844e-01 | 6.3777e-06 | 2.2705e-02 | 10,485,760\n",
      "model.layers.12.self_attn.q_norm.weight       | -1.5259e-02 | 4.7188e+00 | 1.6172e+00 | 5.2344e-01 |      128\n",
      "model.layers.12.self_attn.k_norm.weight       | -8.2397e-03 | 5.8438e+00 | 1.6797e+00 | 7.5781e-01 |      128\n",
      "model.layers.12.mlp.gate_proj.weight          | -6.6797e-01 | 6.3281e-01 | 1.6928e-05 | 2.5024e-02 | 24,903,680\n",
      "model.layers.12.mlp.up_proj.weight            | -6.2500e-01 | 5.6641e-01 | -7.5996e-06 | 2.3193e-02 | 24,903,680\n",
      "model.layers.12.mlp.down_proj.weight          | -7.5000e-01 | 9.6094e-01 | -3.7849e-06 | 2.2705e-02 | 24,903,680\n",
      "model.layers.12.input_layernorm.weight        | 6.9336e-02 | 1.0625e+00 | 3.2031e-01 | 8.2031e-02 |    2,560\n",
      "model.layers.12.post_attention_layernorm.weight | -3.5352e-01 | 1.0469e+00 | 5.8984e-01 | 1.6797e-01 |    2,560\n",
      "model.layers.13.self_attn.q_proj.weight       | -3.3984e-01 | 3.5938e-01 | 8.4564e-07 | 2.2705e-02 | 10,485,760\n",
      "model.layers.13.self_attn.k_proj.weight       | -5.0391e-01 | 6.5234e-01 | -3.0518e-05 | 2.2461e-02 | 2,621,440\n",
      "model.layers.13.self_attn.v_proj.weight       | -2.1973e-01 | 1.7285e-01 | -3.5167e-06 | 2.5146e-02 | 2,621,440\n",
      "model.layers.13.self_attn.o_proj.weight       | -4.1797e-01 | 3.8867e-01 | -5.7518e-06 | 2.2217e-02 | 10,485,760\n",
      "model.layers.13.self_attn.q_norm.weight       | -1.1414e-02 | 3.7656e+00 | 1.7500e+00 | 4.5703e-01 |      128\n",
      "model.layers.13.self_attn.k_norm.weight       | -3.9307e-02 | 3.2656e+00 | 1.6797e+00 | 6.7188e-01 |      128\n",
      "model.layers.13.mlp.gate_proj.weight          | -4.7070e-01 | 4.5703e-01 | 4.0054e-05 | 2.4170e-02 | 24,903,680\n",
      "model.layers.13.mlp.up_proj.weight            | -6.9141e-01 | 8.2422e-01 | 3.9339e-06 | 2.3560e-02 | 24,903,680\n",
      "model.layers.13.mlp.down_proj.weight          | -9.5312e-01 | 7.7344e-01 | -2.6226e-06 | 2.2827e-02 | 24,903,680\n",
      "model.layers.13.input_layernorm.weight        | 4.8584e-02 | 8.9844e-01 | 2.8516e-01 | 8.8867e-02 |    2,560\n",
      "model.layers.13.post_attention_layernorm.weight | 1.2457e-05 | 1.2266e+00 | 5.8984e-01 | 1.6309e-01 |    2,560\n",
      "model.layers.14.self_attn.q_proj.weight       | -3.2031e-01 | 3.2031e-01 | 1.5259e-05 | 2.2705e-02 | 10,485,760\n",
      "model.layers.14.self_attn.k_proj.weight       | -3.7305e-01 | 3.3398e-01 | 1.3784e-06 | 2.2583e-02 | 2,621,440\n",
      "model.layers.14.self_attn.v_proj.weight       | -1.5332e-01 | 1.7285e-01 | -3.0994e-06 | 2.4536e-02 | 2,621,440\n",
      "model.layers.14.self_attn.o_proj.weight       | -5.0000e-01 | 4.9219e-01 | -1.6019e-06 | 2.1851e-02 | 10,485,760\n",
      "model.layers.14.self_attn.q_norm.weight       | 2.7008e-03 | 3.7031e+00 | 1.6641e+00 | 4.0430e-01 |      128\n",
      "model.layers.14.self_attn.k_norm.weight       | 1.0452e-03 | 8.1875e+00 | 1.7578e+00 | 8.0469e-01 |      128\n",
      "model.layers.14.mlp.gate_proj.weight          | -5.5078e-01 | 5.8203e-01 | 5.5313e-05 | 2.3804e-02 | 24,903,680\n",
      "model.layers.14.mlp.up_proj.weight            | -4.1992e-01 | 4.5703e-01 | 4.0531e-06 | 2.3193e-02 | 24,903,680\n",
      "model.layers.14.mlp.down_proj.weight          | -1.0547e+00 | 6.8750e-01 | -1.5646e-07 | 2.2583e-02 | 24,903,680\n",
      "model.layers.14.input_layernorm.weight        | 5.1270e-02 | 1.2422e+00 | 3.9258e-01 | 1.3574e-01 |    2,560\n",
      "model.layers.14.post_attention_layernorm.weight | -3.6240e-05 | 1.3906e+00 | 6.0547e-01 | 1.8652e-01 |    2,560\n",
      "model.layers.15.self_attn.q_proj.weight       | -3.3789e-01 | 3.3398e-01 | -4.4405e-06 | 2.3071e-02 | 10,485,760\n",
      "model.layers.15.self_attn.k_proj.weight       | -3.0859e-01 | 3.5156e-01 | -4.7088e-06 | 2.2583e-02 | 2,621,440\n",
      "model.layers.15.self_attn.v_proj.weight       | -1.6113e-01 | 1.8555e-01 | 1.6809e-05 | 2.4292e-02 | 2,621,440\n",
      "model.layers.15.self_attn.o_proj.weight       | -3.1836e-01 | 3.7109e-01 | 1.0207e-06 | 2.1240e-02 | 10,485,760\n",
      "model.layers.15.self_attn.q_norm.weight       | -1.2817e-02 | 2.3281e+00 | 1.7031e+00 | 3.9648e-01 |      128\n",
      "model.layers.15.self_attn.k_norm.weight       | -5.2246e-02 | 4.0938e+00 | 1.8281e+00 | 6.4062e-01 |      128\n",
      "model.layers.15.mlp.gate_proj.weight          | -6.1719e-01 | 5.8984e-01 | 3.2902e-05 | 2.3071e-02 | 24,903,680\n",
      "model.layers.15.mlp.up_proj.weight            | -4.6094e-01 | 3.6328e-01 | -3.7700e-06 | 2.2949e-02 | 24,903,680\n",
      "model.layers.15.mlp.down_proj.weight          | -5.1953e-01 | 8.2422e-01 | 2.6375e-06 | 2.2461e-02 | 24,903,680\n",
      "model.layers.15.input_layernorm.weight        | 4.1748e-02 | 1.3516e+00 | 4.0430e-01 | 1.7188e-01 |    2,560\n",
      "model.layers.15.post_attention_layernorm.weight | -2.5586e-01 | 1.5859e+00 | 6.1328e-01 | 2.0801e-01 |    2,560\n",
      "model.layers.16.self_attn.q_proj.weight       | -3.3594e-01 | 3.2227e-01 | -9.8348e-06 | 2.2339e-02 | 10,485,760\n",
      "model.layers.16.self_attn.k_proj.weight       | -3.4375e-01 | 4.3164e-01 | -1.4782e-05 | 2.1851e-02 | 2,621,440\n",
      "model.layers.16.self_attn.v_proj.weight       | -1.5527e-01 | 1.5137e-01 | 6.0797e-06 | 2.4292e-02 | 2,621,440\n",
      "model.layers.16.self_attn.o_proj.weight       | -6.8750e-01 | 6.1328e-01 | -2.5034e-06 | 2.1851e-02 | 10,485,760\n",
      "model.layers.16.self_attn.q_norm.weight       | 1.9165e-02 | 3.9375e+00 | 1.5391e+00 | 4.4336e-01 |      128\n",
      "model.layers.16.self_attn.k_norm.weight       | -1.0681e-02 | 5.3438e+00 | 1.6719e+00 | 7.1094e-01 |      128\n",
      "model.layers.16.mlp.gate_proj.weight          | -4.8633e-01 | 5.4297e-01 | 3.6955e-05 | 2.3560e-02 | 24,903,680\n",
      "model.layers.16.mlp.up_proj.weight            | -8.5938e-01 | 1.0078e+00 | -7.2718e-06 | 2.2705e-02 | 24,903,680\n",
      "model.layers.16.mlp.down_proj.weight          | -1.2266e+00 | 1.1406e+00 | 7.4878e-07 | 2.2095e-02 | 24,903,680\n",
      "model.layers.16.input_layernorm.weight        | 4.2236e-02 | 1.5391e+00 | 5.7031e-01 | 2.7930e-01 |    2,560\n",
      "model.layers.16.post_attention_layernorm.weight | -1.7047e-05 | 2.7500e+00 | 6.3672e-01 | 2.2266e-01 |    2,560\n",
      "model.layers.17.self_attn.q_proj.weight       | -3.2812e-01 | 2.8906e-01 | 7.5102e-06 | 2.2949e-02 | 10,485,760\n",
      "model.layers.17.self_attn.k_proj.weight       | -2.5977e-01 | 2.6562e-01 | -3.5912e-06 | 2.1240e-02 | 2,621,440\n",
      "model.layers.17.self_attn.v_proj.weight       | -1.5625e-01 | 1.8262e-01 | 1.2338e-05 | 2.3682e-02 | 2,621,440\n",
      "model.layers.17.self_attn.o_proj.weight       | -4.2969e-01 | 4.8633e-01 | -3.6806e-06 | 2.1484e-02 | 10,485,760\n",
      "model.layers.17.self_attn.q_norm.weight       | -8.3008e-02 | 2.7500e+00 | 1.6797e+00 | 4.8633e-01 |      128\n",
      "model.layers.17.self_attn.k_norm.weight       | -5.8105e-02 | 3.6719e+00 | 1.7578e+00 | 7.4219e-01 |      128\n",
      "model.layers.17.mlp.gate_proj.weight          | -5.8203e-01 | 6.3281e-01 | 2.3007e-05 | 2.3315e-02 | 24,903,680\n",
      "model.layers.17.mlp.up_proj.weight            | -5.0391e-01 | 4.8633e-01 | 5.6922e-06 | 2.2827e-02 | 24,903,680\n",
      "model.layers.17.mlp.down_proj.weight          | -7.1875e-01 | 7.4219e-01 | 1.3560e-06 | 2.2095e-02 | 24,903,680\n",
      "model.layers.17.input_layernorm.weight        | 4.4189e-02 | 1.5078e+00 | 5.3125e-01 | 2.5000e-01 |    2,560\n",
      "model.layers.17.post_attention_layernorm.weight | -2.6172e-01 | 1.2500e+00 | 6.2500e-01 | 2.1387e-01 |    2,560\n",
      "model.layers.18.self_attn.q_proj.weight       | -3.1055e-01 | 3.3398e-01 | -3.0398e-06 | 2.2217e-02 | 10,485,760\n",
      "model.layers.18.self_attn.k_proj.weight       | -2.6367e-01 | 2.3047e-01 | -2.5630e-06 | 2.1484e-02 | 2,621,440\n",
      "model.layers.18.self_attn.v_proj.weight       | -1.7578e-01 | 1.4746e-01 | -1.8001e-05 | 2.4170e-02 | 2,621,440\n",
      "model.layers.18.self_attn.o_proj.weight       | -4.0430e-01 | 3.8867e-01 | 2.9206e-06 | 2.1484e-02 | 10,485,760\n",
      "model.layers.18.self_attn.q_norm.weight       | 4.0283e-03 | 3.9844e+00 | 1.6641e+00 | 4.1992e-01 |      128\n",
      "model.layers.18.self_attn.k_norm.weight       | -1.0938e-01 | 6.0000e+00 | 1.7422e+00 | 7.1094e-01 |      128\n",
      "model.layers.18.mlp.gate_proj.weight          | -6.4453e-01 | 5.7031e-01 | 2.1815e-05 | 2.2949e-02 | 24,903,680\n",
      "model.layers.18.mlp.up_proj.weight            | -6.6016e-01 | 5.4688e-01 | 9.2387e-06 | 2.2827e-02 | 24,903,680\n",
      "model.layers.18.mlp.down_proj.weight          | -7.5391e-01 | 7.3438e-01 | 6.1393e-06 | 2.2095e-02 | 24,903,680\n",
      "model.layers.18.input_layernorm.weight        | 4.4922e-02 | 1.9219e+00 | 5.6250e-01 | 2.6953e-01 |    2,560\n",
      "model.layers.18.post_attention_layernorm.weight | -3.3008e-01 | 1.2500e+00 | 6.4062e-01 | 2.0508e-01 |    2,560\n",
      "model.layers.19.self_attn.q_proj.weight       | -3.9648e-01 | 3.3984e-01 | 5.5432e-06 | 2.2583e-02 | 10,485,760\n",
      "model.layers.19.self_attn.k_proj.weight       | -3.8477e-01 | 4.0820e-01 | 8.1658e-06 | 2.0752e-02 | 2,621,440\n",
      "model.layers.19.self_attn.v_proj.weight       | -1.5723e-01 | 1.7090e-01 | -2.2501e-06 | 2.3682e-02 | 2,621,440\n",
      "model.layers.19.self_attn.o_proj.weight       | -4.9414e-01 | 5.5859e-01 | -1.2740e-06 | 2.1729e-02 | 10,485,760\n",
      "model.layers.19.self_attn.q_norm.weight       | -2.9907e-03 | 3.5156e+00 | 1.7422e+00 | 4.6094e-01 |      128\n",
      "model.layers.19.self_attn.k_norm.weight       | -2.7008e-03 | 3.5156e+00 | 1.8516e+00 | 7.1875e-01 |      128\n",
      "model.layers.19.mlp.gate_proj.weight          | -6.7188e-01 | 5.1172e-01 | 4.5598e-06 | 2.2827e-02 | 24,903,680\n",
      "model.layers.19.mlp.up_proj.weight            | -1.0703e+00 | 5.7812e-01 | 5.6252e-07 | 2.2827e-02 | 24,903,680\n",
      "model.layers.19.mlp.down_proj.weight          | -9.7656e-01 | 6.7188e-01 | -2.6077e-06 | 2.2095e-02 | 24,903,680\n",
      "model.layers.19.input_layernorm.weight        | 1.2012e-01 | 2.4219e+00 | 7.1094e-01 | 2.7930e-01 |    2,560\n",
      "model.layers.19.post_attention_layernorm.weight | -1.4305e-04 | 1.1562e+00 | 6.4453e-01 | 1.8066e-01 |    2,560\n",
      "model.layers.20.self_attn.q_proj.weight       | -3.3203e-01 | 3.4180e-01 | -2.0266e-05 | 2.3315e-02 | 10,485,760\n",
      "model.layers.20.self_attn.k_proj.weight       | -2.5195e-01 | 3.2617e-01 | -2.0385e-05 | 2.0508e-02 | 2,621,440\n",
      "model.layers.20.self_attn.v_proj.weight       | -1.8457e-01 | 1.7773e-01 | 2.2948e-06 | 2.3560e-02 | 2,621,440\n",
      "model.layers.20.self_attn.o_proj.weight       | -4.2188e-01 | 4.1992e-01 | 5.0664e-06 | 2.1118e-02 | 10,485,760\n",
      "model.layers.20.self_attn.q_norm.weight       | -9.8267e-03 | 2.8438e+00 | 1.7344e+00 | 4.9219e-01 |      128\n",
      "model.layers.20.self_attn.k_norm.weight       | -6.7871e-02 | 3.3750e+00 | 1.8359e+00 | 6.7969e-01 |      128\n",
      "model.layers.20.mlp.gate_proj.weight          | -5.0000e-01 | 5.0000e-01 | 5.5730e-06 | 2.2583e-02 | 24,903,680\n",
      "model.layers.20.mlp.up_proj.weight            | -4.6094e-01 | 8.2812e-01 | -3.5018e-06 | 2.2949e-02 | 24,903,680\n",
      "model.layers.20.mlp.down_proj.weight          | -4.1602e-01 | 4.5117e-01 | -7.9870e-06 | 2.2217e-02 | 24,903,680\n",
      "model.layers.20.input_layernorm.weight        | 1.0059e-01 | 2.2656e+00 | 6.4844e-01 | 2.4609e-01 |    2,560\n",
      "model.layers.20.post_attention_layernorm.weight | -1.5140e-05 | 1.2422e+00 | 6.7188e-01 | 1.8066e-01 |    2,560\n",
      "model.layers.21.self_attn.q_proj.weight       | -3.5156e-01 | 3.1641e-01 | 5.9232e-07 | 2.2827e-02 | 10,485,760\n",
      "model.layers.21.self_attn.k_proj.weight       | -2.4121e-01 | 2.0703e-01 | -2.2799e-06 | 2.0508e-02 | 2,621,440\n",
      "model.layers.21.self_attn.v_proj.weight       | -2.7344e-01 | 2.7930e-01 | -1.9372e-06 | 2.3193e-02 | 2,621,440\n",
      "model.layers.21.self_attn.o_proj.weight       | -5.3516e-01 | 2.3828e-01 | 5.6997e-07 | 2.0874e-02 | 10,485,760\n",
      "model.layers.21.self_attn.q_norm.weight       | -3.5095e-04 | 5.5625e+00 | 1.7344e+00 | 5.2734e-01 |      128\n",
      "model.layers.21.self_attn.k_norm.weight       | -4.8096e-02 | 4.7812e+00 | 1.8203e+00 | 6.6016e-01 |      128\n",
      "model.layers.21.mlp.gate_proj.weight          | -6.6797e-01 | 4.9023e-01 | 6.7651e-06 | 2.2217e-02 | 24,903,680\n",
      "model.layers.21.mlp.up_proj.weight            | -2.9102e-01 | 3.9844e-01 | -3.3081e-06 | 2.2827e-02 | 24,903,680\n",
      "model.layers.21.mlp.down_proj.weight          | -7.5781e-01 | 7.5781e-01 | 1.3709e-06 | 2.2461e-02 | 24,903,680\n",
      "model.layers.21.input_layernorm.weight        | 1.4746e-01 | 2.7500e+00 | 7.1875e-01 | 2.5000e-01 |    2,560\n",
      "model.layers.21.post_attention_layernorm.weight | -1.0490e-04 | 1.2734e+00 | 7.0703e-01 | 1.6113e-01 |    2,560\n",
      "model.layers.22.self_attn.q_proj.weight       | -3.1836e-01 | 3.2031e-01 | 1.4246e-05 | 2.2461e-02 | 10,485,760\n",
      "model.layers.22.self_attn.k_proj.weight       | -7.3047e-01 | 6.6016e-01 | -1.9222e-06 | 1.9775e-02 | 2,621,440\n",
      "model.layers.22.self_attn.v_proj.weight       | -2.5000e-01 | 2.4023e-01 | -1.3769e-05 | 2.3193e-02 | 2,621,440\n",
      "model.layers.22.self_attn.o_proj.weight       | -4.7266e-01 | 4.1992e-01 | 1.1772e-06 | 2.1240e-02 | 10,485,760\n",
      "model.layers.22.self_attn.q_norm.weight       | -9.8419e-04 | 2.9219e+00 | 1.6406e+00 | 4.5117e-01 |      128\n",
      "model.layers.22.self_attn.k_norm.weight       | -1.0010e-02 | 6.3438e+00 | 1.7812e+00 | 8.2031e-01 |      128\n",
      "model.layers.22.mlp.gate_proj.weight          | -7.7344e-01 | 6.0156e-01 | 9.9540e-06 | 2.3071e-02 | 24,903,680\n",
      "model.layers.22.mlp.up_proj.weight            | -5.6641e-01 | 9.5312e-01 | -1.2219e-06 | 2.3438e-02 | 24,903,680\n",
      "model.layers.22.mlp.down_proj.weight          | -7.1094e-01 | 8.7500e-01 | -3.1143e-06 | 2.2583e-02 | 24,903,680\n",
      "model.layers.22.input_layernorm.weight        | 2.4316e-01 | 3.6875e+00 | 9.8047e-01 | 2.7148e-01 |    2,560\n",
      "model.layers.22.post_attention_layernorm.weight | -7.6294e-05 | 1.2031e+00 | 7.1875e-01 | 1.3574e-01 |    2,560\n",
      "model.layers.23.self_attn.q_proj.weight       | -3.4180e-01 | 4.2188e-01 | 7.8678e-06 | 2.3926e-02 | 10,485,760\n",
      "model.layers.23.self_attn.k_proj.weight       | -2.5781e-01 | 2.5781e-01 | 6.1095e-06 | 2.0874e-02 | 2,621,440\n",
      "model.layers.23.self_attn.v_proj.weight       | -1.9824e-01 | 2.0410e-01 | -2.3693e-06 | 2.3560e-02 | 2,621,440\n",
      "model.layers.23.self_attn.o_proj.weight       | -3.1641e-01 | 3.4570e-01 | 6.1095e-06 | 2.2095e-02 | 10,485,760\n",
      "model.layers.23.self_attn.q_norm.weight       | -1.1169e-02 | 2.9062e+00 | 1.7656e+00 | 4.9219e-01 |      128\n",
      "model.layers.23.self_attn.k_norm.weight       | -5.1270e-03 | 7.3750e+00 | 1.9453e+00 | 8.4375e-01 |      128\n",
      "model.layers.23.mlp.gate_proj.weight          | -4.9805e-01 | 4.0430e-01 | -1.0908e-05 | 2.3804e-02 | 24,903,680\n",
      "model.layers.23.mlp.up_proj.weight            | -3.5156e-01 | 3.2617e-01 | -1.9073e-06 | 2.3438e-02 | 24,903,680\n",
      "model.layers.23.mlp.down_proj.weight          | -5.2734e-01 | 4.8633e-01 | -1.7583e-06 | 2.2705e-02 | 24,903,680\n",
      "model.layers.23.input_layernorm.weight        | 2.7734e-01 | 3.3125e+00 | 9.2188e-01 | 1.9727e-01 |    2,560\n",
      "model.layers.23.post_attention_layernorm.weight | -9.2030e-05 | 1.3828e+00 | 7.6562e-01 | 1.4062e-01 |    2,560\n",
      "model.layers.24.self_attn.q_proj.weight       | -3.0273e-01 | 3.4570e-01 | -3.0994e-06 | 2.4170e-02 | 10,485,760\n",
      "model.layers.24.self_attn.k_proj.weight       | -2.4316e-01 | 2.4902e-01 | 2.4438e-05 | 2.0874e-02 | 2,621,440\n",
      "model.layers.24.self_attn.v_proj.weight       | -2.5586e-01 | 2.5586e-01 | 6.3479e-06 | 2.3315e-02 | 2,621,440\n",
      "model.layers.24.self_attn.o_proj.weight       | -3.3398e-01 | 3.4570e-01 | -3.4273e-06 | 2.2461e-02 | 10,485,760\n",
      "model.layers.24.self_attn.q_norm.weight       | -6.1768e-02 | 2.8594e+00 | 1.6953e+00 | 4.9414e-01 |      128\n",
      "model.layers.24.self_attn.k_norm.weight       | -3.5645e-02 | 3.5000e+00 | 1.7500e+00 | 7.3828e-01 |      128\n",
      "model.layers.24.mlp.gate_proj.weight          | -5.1172e-01 | 4.1602e-01 | 2.3127e-05 | 2.4292e-02 | 24,903,680\n",
      "model.layers.24.mlp.up_proj.weight            | -3.1836e-01 | 3.7500e-01 | -7.0333e-06 | 2.3560e-02 | 24,903,680\n",
      "model.layers.24.mlp.down_proj.weight          | -4.8047e-01 | 6.2500e-01 | -8.9034e-07 | 2.2827e-02 | 24,903,680\n",
      "model.layers.24.input_layernorm.weight        | 3.7109e-01 | 4.1875e+00 | 1.0781e+00 | 2.0410e-01 |    2,560\n",
      "model.layers.24.post_attention_layernorm.weight | -1.2779e-04 | 1.5547e+00 | 8.2422e-01 | 1.5234e-01 |    2,560\n",
      "model.layers.25.self_attn.q_proj.weight       | -2.9883e-01 | 2.9297e-01 | -1.7047e-05 | 2.2827e-02 | 10,485,760\n",
      "model.layers.25.self_attn.k_proj.weight       | -2.3926e-01 | 2.4414e-01 | 9.5367e-06 | 2.1484e-02 | 2,621,440\n",
      "model.layers.25.self_attn.v_proj.weight       | -1.9922e-01 | 1.9922e-01 | 8.2329e-07 | 2.4414e-02 | 2,621,440\n",
      "model.layers.25.self_attn.o_proj.weight       | -1.9141e-01 | 1.9043e-01 | 4.4107e-06 | 2.2095e-02 | 10,485,760\n",
      "model.layers.25.self_attn.q_norm.weight       | -3.9368e-03 | 4.0000e+00 | 1.7266e+00 | 4.4336e-01 |      128\n",
      "model.layers.25.self_attn.k_norm.weight       | 9.0408e-04 | 4.9688e+00 | 1.8047e+00 | 6.9531e-01 |      128\n",
      "model.layers.25.mlp.gate_proj.weight          | -8.3203e-01 | 3.9453e-01 | 9.2983e-06 | 2.4536e-02 | 24,903,680\n",
      "model.layers.25.mlp.up_proj.weight            | -3.6133e-01 | 3.9648e-01 | -2.1309e-06 | 2.3560e-02 | 24,903,680\n",
      "model.layers.25.mlp.down_proj.weight          | -4.5898e-01 | 4.6484e-01 | -2.8163e-06 | 2.2949e-02 | 24,903,680\n",
      "model.layers.25.input_layernorm.weight        | 3.6328e-01 | 4.0938e+00 | 1.0312e+00 | 1.8848e-01 |    2,560\n",
      "model.layers.25.post_attention_layernorm.weight | -2.8372e-05 | 1.5625e+00 | 8.6719e-01 | 1.4746e-01 |    2,560\n",
      "model.layers.26.self_attn.q_proj.weight       | -3.2422e-01 | 3.2617e-01 | -6.7353e-06 | 2.3193e-02 | 10,485,760\n",
      "model.layers.26.self_attn.k_proj.weight       | -2.3633e-01 | 2.4219e-01 | 2.2173e-05 | 2.1240e-02 | 2,621,440\n",
      "model.layers.26.self_attn.v_proj.weight       | -1.7969e-01 | 1.8457e-01 | -1.8328e-06 | 2.4170e-02 | 2,621,440\n",
      "model.layers.26.self_attn.o_proj.weight       | -2.2461e-01 | 2.3340e-01 | 7.4133e-07 | 2.2583e-02 | 10,485,760\n",
      "model.layers.26.self_attn.q_norm.weight       | -3.2471e-02 | 3.9219e+00 | 1.6562e+00 | 4.7461e-01 |      128\n",
      "model.layers.26.self_attn.k_norm.weight       | -1.3977e-02 | 4.3438e+00 | 1.6406e+00 | 7.0703e-01 |      128\n",
      "model.layers.26.mlp.gate_proj.weight          | -5.8984e-01 | 5.3906e-01 | 1.4603e-05 | 2.4658e-02 | 24,903,680\n",
      "model.layers.26.mlp.up_proj.weight            | -8.1641e-01 | 4.9219e-01 | 2.7567e-06 | 2.3804e-02 | 24,903,680\n",
      "model.layers.26.mlp.down_proj.weight          | -7.1484e-01 | 4.3555e-01 | 4.4107e-06 | 2.3315e-02 | 24,903,680\n",
      "model.layers.26.input_layernorm.weight        | 3.9062e-01 | 5.0938e+00 | 1.2656e+00 | 2.4121e-01 |    2,560\n",
      "model.layers.26.post_attention_layernorm.weight | -1.2684e-04 | 1.6797e+00 | 9.3359e-01 | 1.5527e-01 |    2,560\n",
      "model.layers.27.self_attn.q_proj.weight       | -3.8086e-01 | 3.0664e-01 | -1.0192e-05 | 2.2949e-02 | 10,485,760\n",
      "model.layers.27.self_attn.k_proj.weight       | -2.3926e-01 | 2.1484e-01 | -9.2387e-06 | 2.0386e-02 | 2,621,440\n",
      "model.layers.27.self_attn.v_proj.weight       | -2.2168e-01 | 2.2852e-01 | 1.3590e-05 | 2.3438e-02 | 2,621,440\n",
      "model.layers.27.self_attn.o_proj.weight       | -2.9688e-01 | 2.8906e-01 | -8.2701e-07 | 2.2095e-02 | 10,485,760\n",
      "model.layers.27.self_attn.q_norm.weight       | -3.1494e-02 | 4.3750e+00 | 1.6250e+00 | 4.4922e-01 |      128\n",
      "model.layers.27.self_attn.k_norm.weight       | -7.2021e-03 | 4.1875e+00 | 1.5938e+00 | 6.7969e-01 |      128\n",
      "model.layers.27.mlp.gate_proj.weight          | -4.9805e-01 | 3.9453e-01 | 1.6212e-05 | 2.4536e-02 | 24,903,680\n",
      "model.layers.27.mlp.up_proj.weight            | -3.5156e-01 | 4.5312e-01 | -2.7418e-06 | 2.4170e-02 | 24,903,680\n",
      "model.layers.27.mlp.down_proj.weight          | -4.1211e-01 | 4.9609e-01 | 7.3761e-07 | 2.3560e-02 | 24,903,680\n",
      "model.layers.27.input_layernorm.weight        | 4.4531e-01 | 6.1562e+00 | 1.3906e+00 | 2.7539e-01 |    2,560\n",
      "model.layers.27.post_attention_layernorm.weight | -8.1539e-05 | 1.8203e+00 | 9.7656e-01 | 1.5039e-01 |    2,560\n",
      "model.layers.28.self_attn.q_proj.weight       | -3.4570e-01 | 3.2812e-01 | -4.3958e-07 | 2.3438e-02 | 10,485,760\n",
      "model.layers.28.self_attn.k_proj.weight       | -2.1777e-01 | 2.3047e-01 | -1.2591e-06 | 2.1118e-02 | 2,621,440\n",
      "model.layers.28.self_attn.v_proj.weight       | -1.7090e-01 | 1.6699e-01 | 3.1292e-06 | 2.4658e-02 | 2,621,440\n",
      "model.layers.28.self_attn.o_proj.weight       | -3.1055e-01 | 3.5352e-01 | -9.0003e-06 | 2.1973e-02 | 10,485,760\n",
      "model.layers.28.self_attn.q_norm.weight       | 4.5471e-03 | 5.0938e+00 | 1.7031e+00 | 5.5078e-01 |      128\n",
      "model.layers.28.self_attn.k_norm.weight       | -3.9368e-03 | 4.7188e+00 | 1.7266e+00 | 5.9375e-01 |      128\n",
      "model.layers.28.mlp.gate_proj.weight          | -5.8594e-01 | 4.3750e-01 | 3.1471e-05 | 2.4292e-02 | 24,903,680\n",
      "model.layers.28.mlp.up_proj.weight            | -5.0781e-01 | 6.7188e-01 | -1.0878e-06 | 2.4414e-02 | 24,903,680\n",
      "model.layers.28.mlp.down_proj.weight          | -5.5859e-01 | 7.4609e-01 | -1.1474e-06 | 2.3804e-02 | 24,903,680\n",
      "model.layers.28.input_layernorm.weight        | 5.8594e-01 | 7.3750e+00 | 1.3281e+00 | 3.7891e-01 |    2,560\n",
      "model.layers.28.post_attention_layernorm.weight | -2.5392e-05 | 1.9531e+00 | 1.0234e+00 | 1.5039e-01 |    2,560\n",
      "model.layers.29.self_attn.q_proj.weight       | -3.2617e-01 | 3.6328e-01 | -6.4075e-06 | 2.4658e-02 | 10,485,760\n",
      "model.layers.29.self_attn.k_proj.weight       | -3.0078e-01 | 2.6953e-01 | 7.7486e-07 | 2.0630e-02 | 2,621,440\n",
      "model.layers.29.self_attn.v_proj.weight       | -3.3203e-01 | 2.3242e-01 | -1.4961e-05 | 2.3560e-02 | 2,621,440\n",
      "model.layers.29.self_attn.o_proj.weight       | -2.1973e-01 | 2.2070e-01 | -2.5928e-06 | 2.2827e-02 | 10,485,760\n",
      "model.layers.29.self_attn.q_norm.weight       | -7.3730e-02 | 3.0000e+00 | 1.5938e+00 | 5.4297e-01 |      128\n",
      "model.layers.29.self_attn.k_norm.weight       | -9.5215e-02 | 4.6562e+00 | 1.5938e+00 | 8.3594e-01 |      128\n",
      "model.layers.29.mlp.gate_proj.weight          | -6.9141e-01 | 5.7422e-01 | 2.1935e-05 | 2.4292e-02 | 24,903,680\n",
      "model.layers.29.mlp.up_proj.weight            | -4.0820e-01 | 3.9062e-01 | 4.1425e-06 | 2.4780e-02 | 24,903,680\n",
      "model.layers.29.mlp.down_proj.weight          | -5.3516e-01 | 4.7656e-01 | -1.5795e-06 | 2.4170e-02 | 24,903,680\n",
      "model.layers.29.input_layernorm.weight        | 5.0391e-01 | 8.9375e+00 | 1.7891e+00 | 5.9375e-01 |    2,560\n",
      "model.layers.29.post_attention_layernorm.weight | -3.7670e-05 | 2.1406e+00 | 1.0938e+00 | 1.7188e-01 |    2,560\n",
      "model.layers.30.self_attn.q_proj.weight       | -3.6328e-01 | 4.9805e-01 | -3.1739e-06 | 2.2583e-02 | 10,485,760\n",
      "model.layers.30.self_attn.k_proj.weight       | -2.9883e-01 | 2.7148e-01 | 1.2696e-05 | 2.1484e-02 | 2,621,440\n",
      "model.layers.30.self_attn.v_proj.weight       | -2.5000e-01 | 2.5977e-01 | 2.3484e-05 | 2.5024e-02 | 2,621,440\n",
      "model.layers.30.self_attn.o_proj.weight       | -2.3340e-01 | 2.4902e-01 | -9.2983e-06 | 2.2705e-02 | 10,485,760\n",
      "model.layers.30.self_attn.q_norm.weight       | 7.0801e-02 | 4.4688e+00 | 1.7891e+00 | 4.7461e-01 |      128\n",
      "model.layers.30.self_attn.k_norm.weight       | -8.5831e-04 | 5.1875e+00 | 1.7656e+00 | 6.6797e-01 |      128\n",
      "model.layers.30.mlp.gate_proj.weight          | -5.7812e-01 | 6.0938e-01 | 2.9802e-05 | 2.4170e-02 | 24,903,680\n",
      "model.layers.30.mlp.up_proj.weight            | -3.3203e-01 | 3.9258e-01 | 1.8850e-06 | 2.5024e-02 | 24,903,680\n",
      "model.layers.30.mlp.down_proj.weight          | -6.6406e-01 | 7.6562e-01 | 4.0419e-07 | 2.4536e-02 | 24,903,680\n",
      "model.layers.30.input_layernorm.weight        | 8.0859e-01 | 1.0312e+01 | 2.2344e+00 | 5.3906e-01 |    2,560\n",
      "model.layers.30.post_attention_layernorm.weight | -7.4863e-05 | 2.0469e+00 | 1.1484e+00 | 1.6406e-01 |    2,560\n",
      "model.layers.31.self_attn.q_proj.weight       | -3.1250e-01 | 4.0625e-01 | -9.8944e-06 | 2.3071e-02 | 10,485,760\n",
      "model.layers.31.self_attn.k_proj.weight       | -2.7148e-01 | 2.8516e-01 | 1.1504e-05 | 2.0630e-02 | 2,621,440\n",
      "model.layers.31.self_attn.v_proj.weight       | -2.0605e-01 | 1.8652e-01 | 1.3828e-05 | 2.5879e-02 | 2,621,440\n",
      "model.layers.31.self_attn.o_proj.weight       | -3.3789e-01 | 3.3398e-01 | -4.5635e-08 | 2.3315e-02 | 10,485,760\n",
      "model.layers.31.self_attn.q_norm.weight       | -3.3264e-03 | 3.1406e+00 | 1.6562e+00 | 4.2578e-01 |      128\n",
      "model.layers.31.self_attn.k_norm.weight       | -1.7334e-02 | 3.3438e+00 | 1.7188e+00 | 7.1094e-01 |      128\n",
      "model.layers.31.mlp.gate_proj.weight          | -6.4453e-01 | 6.3281e-01 | 3.6716e-05 | 2.3804e-02 | 24,903,680\n",
      "model.layers.31.mlp.up_proj.weight            | -4.1016e-01 | 5.7812e-01 | 4.8876e-06 | 2.5391e-02 | 24,903,680\n",
      "model.layers.31.mlp.down_proj.weight          | -7.3828e-01 | 5.0391e-01 | -5.9232e-07 | 2.5024e-02 | 24,903,680\n",
      "model.layers.31.input_layernorm.weight        | 8.5156e-01 | 1.4500e+01 | 2.4062e+00 | 1.0312e+00 |    2,560\n",
      "model.layers.31.post_attention_layernorm.weight | 8.4400e-05 | 1.8984e+00 | 1.1953e+00 | 1.4941e-01 |    2,560\n",
      "model.layers.32.self_attn.q_proj.weight       | -4.4531e-01 | 4.2188e-01 | -1.9372e-06 | 2.2705e-02 | 10,485,760\n",
      "model.layers.32.self_attn.k_proj.weight       | -2.4512e-01 | 2.5000e-01 | 1.8954e-05 | 2.0752e-02 | 2,621,440\n",
      "model.layers.32.self_attn.v_proj.weight       | -2.3047e-01 | 2.3145e-01 | 6.0797e-06 | 2.5879e-02 | 2,621,440\n",
      "model.layers.32.self_attn.o_proj.weight       | -3.0859e-01 | 4.4141e-01 | 3.6657e-06 | 2.3315e-02 | 10,485,760\n",
      "model.layers.32.self_attn.q_norm.weight       | 9.8419e-04 | 4.2500e+00 | 1.7031e+00 | 4.7070e-01 |      128\n",
      "model.layers.32.self_attn.k_norm.weight       | -3.4027e-03 | 4.4062e+00 | 1.7109e+00 | 6.1719e-01 |      128\n",
      "model.layers.32.mlp.gate_proj.weight          | -5.9766e-01 | 6.9922e-01 | 4.3154e-05 | 2.3560e-02 | 24,903,680\n",
      "model.layers.32.mlp.up_proj.weight            | -3.6133e-01 | 8.0078e-01 | 1.2591e-06 | 2.5391e-02 | 24,903,680\n",
      "model.layers.32.mlp.down_proj.weight          | -6.9922e-01 | 6.8750e-01 | 2.2948e-06 | 2.5024e-02 | 24,903,680\n",
      "model.layers.32.input_layernorm.weight        | 1.0938e+00 | 1.8625e+01 | 2.9062e+00 | 1.4219e+00 |    2,560\n",
      "model.layers.32.post_attention_layernorm.weight | -1.0538e-04 | 2.1719e+00 | 1.2422e+00 | 1.3281e-01 |    2,560\n",
      "model.layers.33.self_attn.q_proj.weight       | -3.8867e-01 | 3.6328e-01 | -3.7700e-06 | 2.2827e-02 | 10,485,760\n",
      "model.layers.33.self_attn.k_proj.weight       | -2.1484e-01 | 2.0605e-01 | 1.4246e-05 | 1.8555e-02 | 2,621,440\n",
      "model.layers.33.self_attn.v_proj.weight       | -2.4121e-01 | 2.1191e-01 | 2.0742e-05 | 2.7588e-02 | 2,621,440\n",
      "model.layers.33.self_attn.o_proj.weight       | -2.3633e-01 | 7.0312e-01 | -7.0333e-06 | 2.4292e-02 | 10,485,760\n",
      "model.layers.33.self_attn.q_norm.weight       | -3.7598e-02 | 2.5156e+00 | 1.5391e+00 | 4.8828e-01 |      128\n",
      "model.layers.33.self_attn.k_norm.weight       | -1.9897e-02 | 3.0469e+00 | 1.5391e+00 | 7.7734e-01 |      128\n",
      "model.layers.33.mlp.gate_proj.weight          | -6.1719e-01 | 6.6016e-01 | 2.7299e-05 | 2.3315e-02 | 24,903,680\n",
      "model.layers.33.mlp.up_proj.weight            | -4.4922e-01 | 5.6641e-01 | 1.9819e-06 | 2.5513e-02 | 24,903,680\n",
      "model.layers.33.mlp.down_proj.weight          | -5.6641e-01 | 6.9531e-01 | -2.3544e-06 | 2.4780e-02 | 24,903,680\n",
      "model.layers.33.input_layernorm.weight        | 1.3047e+00 | 2.8000e+01 | 3.6875e+00 | 2.8438e+00 |    2,560\n",
      "model.layers.33.post_attention_layernorm.weight | -4.0233e-06 | 6.2812e+00 | 1.2969e+00 | 1.6992e-01 |    2,560\n",
      "model.layers.34.self_attn.q_proj.weight       | -3.8672e-01 | 4.3164e-01 | 8.9407e-06 | 2.1973e-02 | 10,485,760\n",
      "model.layers.34.self_attn.k_proj.weight       | -2.7344e-01 | 3.4180e-01 | 1.3888e-05 | 2.0020e-02 | 2,621,440\n",
      "model.layers.34.self_attn.v_proj.weight       | -2.4121e-01 | 2.4805e-01 | 2.0862e-05 | 2.7588e-02 | 2,621,440\n",
      "model.layers.34.self_attn.o_proj.weight       | -3.6523e-01 | 3.3398e-01 | 6.4075e-06 | 2.3682e-02 | 10,485,760\n",
      "model.layers.34.self_attn.q_norm.weight       | 6.2561e-03 | 4.7188e+00 | 1.6797e+00 | 5.7422e-01 |      128\n",
      "model.layers.34.self_attn.k_norm.weight       | -1.0223e-03 | 5.0312e+00 | 1.7109e+00 | 6.4453e-01 |      128\n",
      "model.layers.34.mlp.gate_proj.weight          | -6.2500e-01 | 5.5859e-01 | 3.6240e-05 | 2.3682e-02 | 24,903,680\n",
      "model.layers.34.mlp.up_proj.weight            | -7.1094e-01 | 5.3906e-01 | -5.2154e-07 | 2.5146e-02 | 24,903,680\n",
      "model.layers.34.mlp.down_proj.weight          | -7.1875e-01 | 7.6562e-01 | 1.4305e-06 | 2.3438e-02 | 24,903,680\n",
      "model.layers.34.input_layernorm.weight        | 1.5625e+00 | 2.9000e+01 | 4.1250e+00 | 2.2500e+00 |    2,560\n",
      "model.layers.34.post_attention_layernorm.weight | -3.5858e-04 | 2.3250e+01 | 1.4453e+00 | 4.8438e-01 |    2,560\n",
      "model.layers.35.self_attn.q_proj.weight       | -3.2227e-01 | 3.0078e-01 | -6.1691e-06 | 2.2217e-02 | 10,485,760\n",
      "model.layers.35.self_attn.k_proj.weight       | -5.8203e-01 | 6.0547e-01 | -4.4823e-05 | 2.1362e-02 | 2,621,440\n",
      "model.layers.35.self_attn.v_proj.weight       | -1.9434e-01 | 2.0898e-01 | -1.9073e-06 | 2.6001e-02 | 2,621,440\n",
      "model.layers.35.self_attn.o_proj.weight       | -4.8633e-01 | 5.3516e-01 | -2.2203e-06 | 2.2949e-02 | 10,485,760\n",
      "model.layers.35.self_attn.q_norm.weight       | 6.2988e-02 | 4.0000e+00 | 1.6641e+00 | 5.8203e-01 |      128\n",
      "model.layers.35.self_attn.k_norm.weight       | -2.0905e-03 | 8.7500e+00 | 1.8516e+00 | 1.1484e+00 |      128\n",
      "model.layers.35.mlp.gate_proj.weight          | -6.9141e-01 | 8.2812e-01 | 4.7922e-05 | 2.4902e-02 | 24,903,680\n",
      "model.layers.35.mlp.up_proj.weight            | -8.2812e-01 | 6.1328e-01 | 8.0466e-06 | 2.5513e-02 | 24,903,680\n",
      "model.layers.35.mlp.down_proj.weight          | -6.9141e-01 | 7.8516e-01 | -8.4192e-07 | 2.2339e-02 | 24,903,680\n",
      "model.layers.35.input_layernorm.weight        | 2.1191e-01 | 1.7375e+01 | 3.9219e+00 | 1.1875e+00 |    2,560\n",
      "model.layers.35.post_attention_layernorm.weight | 2.9907e-03 | 2.3875e+01 | 1.8594e+00 | 6.3281e-01 |    2,560\n",
      "model.norm.weight                             | -1.4343e-02 | 9.7500e+00 | 2.7500e+00 | 4.5508e-01 |    2,560\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "‚úÖ Total parameters: 4,022,468,096\n",
      "‚úÖ DataFrame shape: (398, 6) (rows=parameters, cols=stats)\n",
      "\n",
      "üí° Example usage:\n",
      "- Mean std across all params: 0.1932\n",
      "- Layer with largest weight magnitude (max |max|):\n",
      "  ‚Üí model.layers.0.self_attn.k_norm.weight (max=44.0000, min=-0.0164)\n",
      "\n",
      "üìä Per-layer-group aggregated stats (numel-weighted avg):\n",
      "             total_params       avg_min      avg_max      avg_avg      avg_std\n",
      "layer_group                                                                   \n",
      "other        4.022468e+09 -5.036800e-01 5.040710e-01 4.700000e-05 2.291100e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3203/2524376040.py:82: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  layer_summary = df.groupby(\"layer_group\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter_name</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>numel</th>\n",
       "      <th>layer_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.embed_tokens.weight</td>\n",
       "      <td>-0.220703</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>-2.646446e-05</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>388956160</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.layers.0.self_attn.q_proj.weight</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>0.439453</td>\n",
       "      <td>6.765127e-06</td>\n",
       "      <td>0.022949</td>\n",
       "      <td>10485760</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.layers.0.self_attn.k_proj.weight</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>1.192093e-05</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>2621440</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.layers.0.self_attn.v_proj.weight</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-9.953976e-06</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>2621440</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.layers.0.self_attn.o_proj.weight</td>\n",
       "      <td>-0.511719</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>-6.705523e-06</td>\n",
       "      <td>0.021362</td>\n",
       "      <td>10485760</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>model.layers.35.mlp.up_proj.weight</td>\n",
       "      <td>-0.828125</td>\n",
       "      <td>0.613281</td>\n",
       "      <td>8.046627e-06</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>24903680</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>model.layers.35.mlp.down_proj.weight</td>\n",
       "      <td>-0.691406</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>-8.419156e-07</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>24903680</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>model.layers.35.input_layernorm.weight</td>\n",
       "      <td>0.211914</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>3.921875e+00</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>2560</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>model.layers.35.post_attention_layernorm.weight</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>23.875000</td>\n",
       "      <td>1.859375e+00</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>2560</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>model.norm.weight</td>\n",
       "      <td>-0.014343</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>2.750000e+00</td>\n",
       "      <td>0.455078</td>\n",
       "      <td>2560</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      parameter_name       min        max  \\\n",
       "0                          model.embed_tokens.weight -0.220703   0.246094   \n",
       "1             model.layers.0.self_attn.q_proj.weight -0.589844   0.439453   \n",
       "2             model.layers.0.self_attn.k_proj.weight -0.292969   0.248047   \n",
       "3             model.layers.0.self_attn.v_proj.weight -0.168945   0.146484   \n",
       "4             model.layers.0.self_attn.o_proj.weight -0.511719   0.511719   \n",
       "..                                               ...       ...        ...   \n",
       "393               model.layers.35.mlp.up_proj.weight -0.828125   0.613281   \n",
       "394             model.layers.35.mlp.down_proj.weight -0.691406   0.785156   \n",
       "395           model.layers.35.input_layernorm.weight  0.211914  17.375000   \n",
       "396  model.layers.35.post_attention_layernorm.weight  0.002991  23.875000   \n",
       "397                                model.norm.weight -0.014343   9.750000   \n",
       "\n",
       "              avg       std      numel layer_group  \n",
       "0   -2.646446e-05  0.021729  388956160       other  \n",
       "1    6.765127e-06  0.022949   10485760       other  \n",
       "2    1.192093e-05  0.024170    2621440       other  \n",
       "3   -9.953976e-06  0.022705    2621440       other  \n",
       "4   -6.705523e-06  0.021362   10485760       other  \n",
       "..            ...       ...        ...         ...  \n",
       "393  8.046627e-06  0.025513   24903680       other  \n",
       "394 -8.419156e-07  0.022339   24903680       other  \n",
       "395  3.921875e+00  1.187500       2560       other  \n",
       "396  1.859375e+00  0.632812       2560       other  \n",
       "397  2.750000e+00  0.455078       2560       other  \n",
       "\n",
       "[398 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_statistics(qwen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ad870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
